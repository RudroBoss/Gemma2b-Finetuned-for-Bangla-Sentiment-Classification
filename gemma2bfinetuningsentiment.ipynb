{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":12265377,"datasetId":7729130,"databundleVersionId":12814203,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"sabbir5622r/july-revolution-sentiment-analysis-dataset-bangla\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:01:47.448275Z","iopub.execute_input":"2026-02-14T09:01:47.448740Z","iopub.status.idle":"2026-02-14T09:01:54.937507Z","shell.execute_reply.started":"2026-02-14T09:01:47.448700Z","shell.execute_reply":"2026-02-14T09:01:54.936599Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/datasets/sabbir5622r/july-revolution-sentiment-analysis-dataset-bangla\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\n\ndata_file = \"/kaggle/input/datasets/sabbir5622r/july-revolution-sentiment-analysis-dataset-bangla/student_people_mass_uprising_public_sentiments_dataset/student_people_mass_uprising_public_sentiments_dataset.csv\"\n\ndf = pd.read_csv(data_file)\n\n# Quick check\nprint(df.head())\nprint(df['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:02:04.211707Z","iopub.execute_input":"2026-02-14T09:02:04.212062Z","iopub.status.idle":"2026-02-14T09:02:04.300008Z","shell.execute_reply.started":"2026-02-14T09:02:04.212029Z","shell.execute_reply":"2026-02-14T09:02:04.299020Z"}},"outputs":[{"name":"stdout","text":"                                             comment language     platform  \\\n0  à¦•à§‡à¦¾à¦¨ à¦¸à¦‚à¦²à¦¾à¦ª à¦¹à¦¬à§‡ à¦¨à¦¾à¥¤ à¦ªà¦¦à¦¤à§à¦¯à¦¾à¦— à¦šà¦¾à¦‡ à¦à¦¬à¦‚ à¦•à¦ à§‡à¦¾à¦° à¦¶à¦¾à¦¸à§à¦¤...   Bangla     Facebook   \n1  à¦¸à¦¬ à¦•à§Ÿà¦Ÿà¦¾ à¦°à¦¾à¦œà¦¨à§ˆà¦¤à¦¿à¦• à¦•à¦°à§‡,à¦à¦°à¦¾ à¦•à§‡à¦‰ à¦¸à¦¾à¦§à¦¾à¦°à¦¨ à¦¶à¦¿à¦•à§à¦·à¦¾à¦°à§à¦¥à§€ à¦¨à§Ÿ   Bangla      Youtube   \n2  à¦…à¦ªà¦°à¦¾à¦§à§‡à¦° à¦•à§‹à¦¨ à¦§à¦°à§à¦® à¦¦à¦²à§€à¦¯à¦¼ à¦ªà¦°à¦¿à¦šà¦¯à¦¼ à¦¨à§‡à¦‡à¥¤ à¦…à¦ªà¦°à¦¾à¦§à§€ à¦°à¦¾à¦·à§...   Bangla     Facebook   \n3               à¦‡à¦‰à¦¨à§à¦¸ à¦¸à¦°à¦•à¦¾à¦° à¦¹à¦¿à¦¨à§à¦¦à§à¦¦à§‡à¦° à¦…à¦¤à§à¦¯à¦¾à¦šà¦¾à¦° à¦•à¦°à¦›à§‡à¥¤   Bangla     Facebook   \n4        à¦›à¦¾à¦¤à§à¦°à¦°à¦¾ à¦¦à§‡à¦¶à§‡à¦° à¦­à¦¬à¦¿à¦·à§à¦¯à§ à¦¨à¦¿à¦°à§à¦®à¦¾à¦£à§‡à¦° à¦®à§‚à¦² à¦•à¦¾à¦°à¦¿à¦—à¦°à¥¤   Bangla  Twitter (X)   \n\n      label  \n0  Positive  \n1  Negative  \n2   Neutral  \n3  Negative  \n4  Positive  \nlabel\nPositive    1400\nNegative    1400\nNeutral     1400\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Map labels directly (dataset e label already string)\nlabel_map = {\"Positive\": \"Positive\", \"Negative\": \"Negative\", \"Neutral\": \"Neutral\"}\n\n# Instruction-response format, correct column name: 'comment'\ndf['text_input'] = \"Classify sentiment: \" + df['comment'].astype(str)\ndf['text_output'] = df['label'].map(label_map)\n\n# Keep only necessary columns\ndf = df[['text_input', 'text_output']]\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:02:09.866966Z","iopub.execute_input":"2026-02-14T09:02:09.867365Z","iopub.status.idle":"2026-02-14T09:02:09.890257Z","shell.execute_reply.started":"2026-02-14T09:02:09.867330Z","shell.execute_reply":"2026-02-14T09:02:09.889101Z"}},"outputs":[{"name":"stdout","text":"                                          text_input text_output\n0  Classify sentiment: à¦•à§‡à¦¾à¦¨ à¦¸à¦‚à¦²à¦¾à¦ª à¦¹à¦¬à§‡ à¦¨à¦¾à¥¤ à¦ªà¦¦à¦¤à§à¦¯à¦¾à¦—...    Positive\n1  Classify sentiment: à¦¸à¦¬ à¦•à§Ÿà¦Ÿà¦¾ à¦°à¦¾à¦œà¦¨à§ˆà¦¤à¦¿à¦• à¦•à¦°à§‡,à¦à¦°à¦¾ à¦•...    Negative\n2  Classify sentiment: à¦…à¦ªà¦°à¦¾à¦§à§‡à¦° à¦•à§‹à¦¨ à¦§à¦°à§à¦® à¦¦à¦²à§€à¦¯à¦¼ à¦ªà¦°à¦¿...     Neutral\n3  Classify sentiment: à¦‡à¦‰à¦¨à§à¦¸ à¦¸à¦°à¦•à¦¾à¦° à¦¹à¦¿à¦¨à§à¦¦à§à¦¦à§‡à¦° à¦…à¦¤à§à¦¯...    Negative\n4  Classify sentiment: à¦›à¦¾à¦¤à§à¦°à¦°à¦¾ à¦¦à§‡à¦¶à§‡à¦° à¦­à¦¬à¦¿à¦·à§à¦¯à§ à¦¨à¦¿à¦°à§...    Positive\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom datasets import Dataset\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n\n# Create HuggingFace Dataset from pandas DataFrame\ndataset = Dataset.from_pandas(df)\n\n# Batched=True hole batch e list ashe, tai list comprehension use korte hobe\ndef tokenize(batch):\n    texts = [inp + \" \" + out for inp, out in zip(batch[\"text_input\"], batch[\"text_output\"])]\n    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=16)\n\n# Apply tokenization\ntokenized_dataset = dataset.map(tokenize, batched=True)\n\n# Check the first tokenized example\nprint(tokenized_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:07:02.628115Z","iopub.execute_input":"2026-02-14T09:07:02.628485Z","iopub.status.idle":"2026-02-14T09:07:04.796569Z","shell.execute_reply.started":"2026-02-14T09:07:02.628454Z","shell.execute_reply":"2026-02-14T09:07:04.795581Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a610d0a621284f258024431b4b0193f6"}},"metadata":{}},{"name":"stdout","text":"{'text_input': 'Classify sentiment: à¦•à§‡à¦¾à¦¨ à¦¸à¦‚à¦²à¦¾à¦ª à¦¹à¦¬à§‡ à¦¨à¦¾à¥¤ à¦ªà¦¦à¦¤à§à¦¯à¦¾à¦— à¦šà¦¾à¦‡ à¦à¦¬à¦‚ à¦•à¦ à§‡à¦¾à¦° à¦¶à¦¾à¦¸à§à¦¤à¦¿ à¦šà¦¾à¦‡', 'text_output': 'Positive', 'input_ids': [2, 212107, 25627, 235292, 24313, 236180, 35243, 119877, 60648, 236955, 44663, 79618, 136624, 235940, 28596, 237273], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q --upgrade peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:45:49.912784Z","iopub.execute_input":"2026-02-14T08:45:49.913227Z","iopub.status.idle":"2026-02-14T08:45:57.472079Z","shell.execute_reply.started":"2026-02-14T08:45:49.913196Z","shell.execute_reply":"2026-02-14T08:45:57.471362Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\nfrom transformers import DataCollatorForLanguageModeling","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:46:51.991849Z","iopub.execute_input":"2026-02-14T08:46:51.992136Z","iopub.status.idle":"2026-02-14T08:47:25.058435Z","shell.execute_reply.started":"2026-02-14T08:46:51.992104Z","shell.execute_reply":"2026-02-14T08:47:25.057609Z"}},"outputs":[{"name":"stderr","text":"2026-02-14 08:47:07.202699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771058827.428785      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771058827.491685      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771058828.032717      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771058828.032752      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771058828.032754      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771058828.032757      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 0ï¸âƒ£ Install required packages (run once in Kaggle)\n!pip install -U torch transformers peft accelerate trl datasets bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:11:58.483970Z","iopub.execute_input":"2026-02-14T09:11:58.484255Z","iopub.status.idle":"2026-02-14T09:14:50.642486Z","shell.execute_reply.started":"2026-02-14T09:11:58.484231Z","shell.execute_reply":"2026-02-14T09:14:50.641781Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nCollecting torch\n  Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nCollecting transformers\n  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\nCollecting peft\n  Downloading peft-0.18.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\nCollecting accelerate\n  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nCollecting trl\n  Downloading trl-0.28.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\nCollecting datasets\n  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\nCollecting cuda-bindings==12.9.4 (from torch)\n  Downloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nCollecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch)\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nCollecting nvidia-nccl-cu12==2.27.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.5)\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch)\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.6.0 (from torch)\n  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nCollecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch)\n  Downloading cuda_pathfinder-1.3.4-py3-none-any.whl.metadata (1.9 kB)\nCollecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.1rc0)\nRequirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nDownloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading peft-0.18.1-py3-none-any.whl (556 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.28.0-py3-none-any.whl (540 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-4.5.0-py3-none-any.whl (515 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m560.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cuda_pathfinder-1.3.4-py3-none-any.whl (30 kB)\nInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, cuda-pathfinder, nvidia-cusparse-cu12, nvidia-cufft-cu12, cuda-bindings, nvidia-cusolver-cu12, huggingface-hub, torch, datasets, transformers, bitsandbytes, accelerate, trl, peft\n  Attempting uninstall: triton\n    Found existing installation: triton 3.4.0\n    Uninstalling triton-3.4.0:\n      Successfully uninstalled triton-3.4.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.6.77\n    Uninstalling nvidia-nvtx-cu12-12.6.77:\n      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.27.3\n    Uninstalling nvidia-nccl-cu12-2.27.3:\n      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufile-cu12\n    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.36.0\n    Uninstalling huggingface-hub-0.36.0:\n      Successfully uninstalled huggingface-hub-0.36.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.8.0+cu126\n    Uninstalling torch-2.8.0+cu126:\n      Successfully uninstalled torch-2.8.0+cu126\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.2\n    Uninstalling datasets-4.4.2:\n      Successfully uninstalled datasets-4.4.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.57.1\n    Uninstalling transformers-4.57.1:\n      Successfully uninstalled transformers-4.57.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.11.0\n    Uninstalling accelerate-1.11.0:\n      Successfully uninstalled accelerate-1.11.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.17.1\n    Uninstalling peft-0.17.1:\n      Successfully uninstalled peft-0.17.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.1.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\ntorchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.10.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.10.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.12.0 bitsandbytes-0.49.1 cuda-bindings-12.9.4 cuda-pathfinder-1.3.4 datasets-4.5.0 huggingface-hub-1.4.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 peft-0.18.1 torch-2.10.0 transformers-5.1.0 triton-3.6.0 trl-0.28.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#MAIN CODE FULL MODEL FINTETUNED\n\nimport os\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n)\nfrom huggingface_hub import login\nimport numpy as np\nimport shutil\nimport gc\n\n# ============================================================================\n# STEP 1: Setup and Authentication\n# ============================================================================\nprint(\"ğŸ” Logging into Hugging Face...\")\nlogin(\"hf_token\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"ğŸ“± Using device: {device}\")\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(f\"âœ“ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# ============================================================================\n# DISK SPACE CHECK\n# ============================================================================\ndef check_disk_space():\n    \"\"\"Check available disk space\"\"\"\n    stat = shutil.disk_usage(\"/\")\n    free_gb = stat.free / (1024**3)\n    print(f\"ğŸ’¾ Free disk space: {free_gb:.2f} GB\")\n    if free_gb < 5:\n        print(\"âš ï¸  WARNING: Low disk space! May cause save errors.\")\n    return free_gb\n\ncheck_disk_space()\n\n# Clean up old checkpoints if they exist\nif os.path.exists(\"./gemma2b_sentiment\"):\n    print(\"ğŸ§¹ Cleaning old checkpoints...\")\n    shutil.rmtree(\"./gemma2b_sentiment\", ignore_errors=True)\n\n# ============================================================================\n# STEP 2: Load Dataset\n# ============================================================================\nprint(\"\\nğŸ“Š Loading dataset...\")\ndata_file = \"/kaggle/input/datasets/sabbir5622r/july-revolution-sentiment-analysis-dataset-bangla/student_people_mass_uprising_public_sentiments_dataset/student_people_mass_uprising_public_sentiments_dataset.csv\"\n\ndf = pd.read_csv(data_file)\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Label distribution:\\n{df['label'].value_counts()}\")\n\n# ============================================================================\n# STEP 3: Format Data\n# ============================================================================\nprint(\"\\nğŸ“ Formatting data...\")\n\ndef format_prompt(row):\n    \"\"\"Simple, consistent format\"\"\"\n    text = str(row['comment']).strip()\n    label = str(row['label']).strip()\n    \n    instruction = f\"Sentiment: {text}\\nLabel:\"\n    response = f\" {label}\"\n    \n    return instruction, response\n\ndf['instruction'] = df.apply(lambda x: format_prompt(x)[0], axis=1)\ndf['response'] = df.apply(lambda x: format_prompt(x)[1], axis=1)\n\n# Proper split with stratification\nfrom sklearn.model_selection import train_test_split\ntrain_df, eval_df = train_test_split(\n    df[['instruction', 'response']], \n    test_size=0.15, \n    random_state=42,\n    stratify=df['label']\n)\n\nprint(f\"Train samples: {len(train_df)}, Eval samples: {len(eval_df)}\")\n\n# ============================================================================\n# STEP 4: Load Tokenizer and Model\n# ============================================================================\nprint(\"\\nğŸ¤– Loading tokenizer and model...\")\nmodel_name = \"google/gemma-2b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\ntokenizer.padding_side = \"right\"\n\nprint(f\"âœ“ Vocab size: {len(tokenizer)}\")\nprint(f\"âœ“ Pad token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n\nuse_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\nprint(f\"âœ“ Using BF16: {use_bf16}\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16 if use_bf16 else torch.float32,\n    device_map=\"auto\",\n)\n\nmodel.config.use_cache = False\nmodel.gradient_checkpointing_enable()\n\nprint(f\"âœ“ Model loaded on: {model.device}\")\n\n# ============================================================================\n# STEP 5: Tokenization with Proper Label Masking\n# ============================================================================\nprint(\"\\nğŸ”¤ Tokenizing...\")\n\ndef tokenize_function(examples):\n    \"\"\"Tokenize with proper label masking\"\"\"\n    model_inputs = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"labels\": []\n    }\n    \n    for instruction, response in zip(examples['instruction'], examples['response']):\n        full_text = instruction + response\n        \n        inst_tokens = tokenizer(\n            instruction,\n            add_special_tokens=True,\n            truncation=True,\n            max_length=110,\n        )\n        \n        full_tokens = tokenizer(\n            full_text,\n            add_special_tokens=True,\n            truncation=True,\n            max_length=128,\n        )\n        \n        inst_len = len(inst_tokens[\"input_ids\"])\n        full_len = len(full_tokens[\"input_ids\"])\n        \n        labels = [-100] * inst_len + full_tokens[\"input_ids\"][inst_len:]\n        \n        # Ensure at least 1 non-masked token\n        non_masked = sum(1 for l in labels if l != -100)\n        if non_masked == 0:\n            labels[-1] = full_tokens[\"input_ids\"][-1]\n        \n        labels = labels[:full_len]\n        \n        model_inputs[\"input_ids\"].append(full_tokens[\"input_ids\"])\n        model_inputs[\"attention_mask\"].append(full_tokens[\"attention_mask\"])\n        model_inputs[\"labels\"].append(labels)\n    \n    return model_inputs\n\ntrain_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\neval_dataset = Dataset.from_pandas(eval_df.reset_index(drop=True))\n\nprint(\"Tokenizing datasets...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=50,\n    remove_columns=train_dataset.column_names,\n)\n\neval_dataset = eval_dataset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=50,\n    remove_columns=eval_dataset.column_names,\n)\n\nprint(f\"âœ“ Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n\n# ============================================================================\n# STEP 6: Data Collator\n# ============================================================================\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List\n\n@dataclass\nclass DataCollatorForSentimentLM:\n    \"\"\"Custom collator with proper padding\"\"\"\n    tokenizer: Any\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        max_length = max(len(f[\"input_ids\"]) for f in features)\n        \n        batch = {\n            \"input_ids\": [],\n            \"attention_mask\": [],\n            \"labels\": []\n        }\n        \n        for feature in features:\n            input_ids = feature[\"input_ids\"]\n            attention_mask = feature[\"attention_mask\"]\n            labels = feature[\"labels\"]\n            \n            padding_length = max_length - len(input_ids)\n            \n            input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n            attention_mask = attention_mask + [0] * padding_length\n            labels = labels + [-100] * padding_length\n            \n            batch[\"input_ids\"].append(input_ids)\n            batch[\"attention_mask\"].append(attention_mask)\n            batch[\"labels\"].append(labels)\n        \n        batch = {\n            k: torch.tensor(v, dtype=torch.long) \n            for k, v in batch.items()\n        }\n        \n        return batch\n\ndata_collator = DataCollatorForSentimentLM(tokenizer=tokenizer)\n\n# ============================================================================\n# STEP 7: OPTIMIZED Training Arguments (DISK SPACE FIX)\n# ============================================================================\nprint(\"\\nâš™ï¸  Setting up training (disk-optimized)...\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./gemma2b_sentiment\",\n    \n    # Training\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    \n    # Learning rate\n    learning_rate=2e-5,\n    warmup_ratio=0.05,\n    lr_scheduler_type=\"cosine\",\n    \n    # Optimization\n    bf16=use_bf16,\n    fp16=False,\n    optim=\"adamw_torch\",\n    max_grad_norm=1.0,\n    \n    # Logging\n    logging_dir=\"./logs\",\n    logging_steps=25,\n    \n    # â­ CRITICAL: Reduced saving to prevent disk errors\n    eval_strategy=\"steps\",\n    eval_steps=200,  # Less frequent (was 100)\n    save_strategy=\"steps\",\n    save_steps=200,  # Less frequent (was 100)\n    save_total_limit=1,  # Keep ONLY 1 checkpoint (was 2)\n    \n    # â­ CRITICAL: Disable optimizer state saving (saves ~2GB per checkpoint)\n    save_only_model=True,  # Don't save optimizer/scheduler states\n    \n    # Other\n    report_to=\"none\",\n    load_best_model_at_end=False,  # Disabled to save space\n    \n    # Memory\n    dataloader_num_workers=0,\n    remove_unused_columns=False,\n    gradient_checkpointing=True,\n    \n    # â­ Disable safetensors temporarily if causing issues\n    save_safetensors=False,  # Use regular pytorch format\n)\n\nprint(f\"âœ“ Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"âœ“ Checkpoints: Every {training_args.save_steps} steps, keeping max {training_args.save_total_limit}\")\n\n# ============================================================================\n# STEP 8: Initialize Trainer\n# ============================================================================\nprint(\"\\nğŸ¯ Initializing Trainer...\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator,\n)\n\nprint(\"âœ“ Trainer ready!\")\n\n# ============================================================================\n# STEP 9: Sanity Check\n# ============================================================================\nprint(\"\\nğŸ” Running sanity check...\")\n\ntry:\n    sample_batch = data_collator([train_dataset[0], train_dataset[1]])\n    sample_batch = {k: v.to(model.device) for k, v in sample_batch.items()}\n    \n    with torch.no_grad():\n        outputs = model(**sample_batch)\n        loss = outputs.loss.item()\n        print(f\"âœ“ Test loss: {loss:.4f} (valid: {not torch.isnan(outputs.loss)})\")\n    \n    del sample_batch, outputs\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(\"âœ… Sanity check passed!\")\n\nexcept Exception as e:\n    print(f\"âŒ Sanity check failed: {e}\")\n    exit(1)\n\n# ============================================================================\n# STEP 10: Training with Cleanup\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸš€ STARTING TRAINING...\")\nprint(\"=\"*70 + \"\\n\")\n\ntry:\n    # Check space before training\n    free_space = check_disk_space()\n    \n    if free_space < 3:\n        print(\"âš ï¸  WARNING: Very low disk space. Training may fail.\")\n        print(\"Consider reducing save_steps or disabling checkpointing entirely.\")\n    \n    train_result = trainer.train()\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"âœ… TRAINING COMPLETED!\")\n    print(\"=\"*70)\n    \n    print(\"\\nğŸ“Š Training Metrics:\")\n    for key, value in train_result.metrics.items():\n        print(f\"  {key}: {value}\")\n    \n    # Save ONLY the final model\n    print(\"\\nğŸ’¾ Saving final model...\")\n    final_dir = \"./gemma2b_sentiment_final\"\n    \n    # Clean up intermediate checkpoints first\n    print(\"ğŸ§¹ Cleaning intermediate checkpoints...\")\n    for item in os.listdir(\"./gemma2b_sentiment\"):\n        if item.startswith(\"checkpoint-\"):\n            shutil.rmtree(f\"./gemma2b_sentiment/{item}\", ignore_errors=True)\n    \n    trainer.save_model(final_dir)\n    tokenizer.save_pretrained(final_dir)\n    print(f\"âœ“ Saved to: {final_dir}\")\n    \n    # Final eval\n    print(\"\\nğŸ“Š Final evaluation...\")\n    eval_results = trainer.evaluate()\n    print(\"\\nFinal Metrics:\")\n    for key, value in eval_results.items():\n        if not np.isnan(value):\n            print(f\"  {key}: {value:.4f}\")\n\nexcept Exception as e:\n    print(f\"\\nâš ï¸  Training encountered error: {e}\")\n    print(\"\\nğŸ’¡ Model may still be usable if training progressed far enough.\")\n    import traceback\n    traceback.print_exc()\n\n# Cleanup\ntorch.cuda.empty_cache()\ngc.collect()\n\n# ============================================================================\n# STEP 11: Test Predictions\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ§ª TESTING PREDICTIONS\")\nprint(\"=\"*70 + \"\\n\")\n\nmodel.eval()\n\ndef predict(text):\n    \"\"\"Predict sentiment\"\"\"\n    prompt = f\"Sentiment: {text}\\nLabel:\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=110).to(device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=5,\n            temperature=0.01,\n            do_sample=False,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    label = result.split(\"Label:\")[-1].strip()\n    \n    del inputs, outputs\n    torch.cuda.empty_cache()\n    \n    return label\n\n# Test samples\ntests = [\n    (\"à¦›à¦¾à¦¤à§à¦°à¦°à¦¾ à¦¦à§‡à¦¶à§‡à¦° à¦­à¦¬à¦¿à¦·à§à¦¯à§\", \"Positive\"),\n    (\"à¦¸à¦°à¦•à¦¾à¦° à¦…à¦¤à§à¦¯à¦¾à¦šà¦¾à¦° à¦•à¦°à¦›à§‡\", \"Negative\"),\n    (\"à¦…à¦ªà¦°à¦¾à¦§à§‡à¦° à¦•à§‹à¦¨ à¦§à¦°à§à¦® à¦¨à§‡à¦‡\", \"Neutral\"),\n    (\"à¦œà§à¦²à¦¾à¦‡ à¦¬à¦¿à¦ªà§à¦²à¦¬ à¦›à¦¿à¦² à¦à¦•à¦Ÿà¦¿ à¦à¦¤à¦¿à¦¹à¦¾à¦¸à¦¿à¦• à¦˜à¦Ÿà¦¨à¦¾\", \"Positive\"),\n    (\"à¦¦à§à¦°à§à¦¨à§€à¦¤à¦¿ à¦¬à¦¨à§à¦§ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡\", \"Negative\"),\n]\n\nprint(\"Testing predictions:\\n\")\ncorrect = 0\nfor text, expected in tests:\n    pred = predict(text)\n    match = expected.lower() in pred.lower()\n    correct += int(match)\n    status = 'âœ“' if match else 'âœ—'\n    print(f\"{status} Text: {text[:50]}...\")\n    print(f\"  Expected: {expected}, Got: {pred}\\n\")\n\nprint(f\"Accuracy: {correct}/{len(tests)} = {100*correct/len(tests):.1f}%\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ‰ DONE!\")\nprint(\"=\"*70)\n\n# Final disk check\ncheck_disk_space()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T11:18:40.279932Z","iopub.execute_input":"2026-02-14T11:18:40.280772Z","iopub.status.idle":"2026-02-14T12:01:41.012565Z","shell.execute_reply.started":"2026-02-14T11:18:40.280735Z","shell.execute_reply":"2026-02-14T12:01:41.011960Z"}},"outputs":[{"name":"stderr","text":"2026-02-14 11:18:56.585508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771067936.792099      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771067936.854609      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771067937.399100      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771067937.399136      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771067937.399138      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771067937.399141      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ” Logging into Hugging Face...\nğŸ“± Using device: cuda\nâœ“ GPU Memory: 15.64 GB\nğŸ’¾ Free disk space: 1426.69 GB\n\nğŸ“Š Loading dataset...\nDataset shape: (4200, 4)\nLabel distribution:\nlabel\nPositive    1400\nNegative    1400\nNeutral     1400\nName: count, dtype: int64\n\nğŸ“ Formatting data...\nTrain samples: 3570, Eval samples: 630\n\nğŸ¤– Loading tokenizer and model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1f8778677644a8f87ea390bc0501296"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"754c454abb944d519c1e634b9580083d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ed54b84b0a4e82af7b1fde09d7063f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3602fc3da7f4aa993de63d3b18eaaa6"}},"metadata":{}},{"name":"stdout","text":"âœ“ Vocab size: 256000\nâœ“ Pad token: '<pad>' (ID: 0)\nâœ“ Using BF16: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160f55abb4b44ee193b9b6e5a7604253"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81993d0ee6124bd58bdc5416291b36b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"256c6aa18190441084800b088dac287d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1e982edd96d43739f51957c5d56e345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6795b9f1a38d4b3eae32bfd8a65fa2ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d25885c774a41278d54416e2cd801df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aabf7ced12614f9ebed3c235f245ec2c"}},"metadata":{}},{"name":"stdout","text":"âœ“ Model loaded on: cuda:0\n\nğŸ”¤ Tokenizing...\nTokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c505d248dab1467da5823b764a5e8c61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/630 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d00c1767379f48c6866d3ba9080139b7"}},"metadata":{}},{"name":"stdout","text":"âœ“ Train: 3570, Eval: 630\n\nâš™ï¸  Setting up training (disk-optimized)...\nâœ“ Effective batch size: 8\nâœ“ Checkpoints: Every 200 steps, keeping max 1\n\nğŸ¯ Initializing Trainer...\nâœ“ Trainer ready!\n\nğŸ” Running sanity check...\nâœ“ Test loss: 8.2857 (valid: True)\nâœ… Sanity check passed!\n\n======================================================================\nğŸš€ STARTING TRAINING...\n======================================================================\n\nğŸ’¾ Free disk space: 1422.00 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='447' max='447' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [447/447 40:05, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.923600</td>\n      <td>0.453245</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.593300</td>\n      <td>0.421569</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n======================================================================\nâœ… TRAINING COMPLETED!\n======================================================================\n\nğŸ“Š Training Metrics:\n  train_runtime: 2411.4058\n  train_samples_per_second: 1.48\n  train_steps_per_second: 0.185\n  total_flos: 2350792381194240.0\n  train_loss: 0.7927555259175482\n  epoch: 1.0\n\nğŸ’¾ Saving final model...\nğŸ§¹ Cleaning intermediate checkpoints...\nâœ“ Saved to: ./gemma2b_sentiment_final\n\nğŸ“Š Final evaluation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [315/315 01:35]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nFinal Metrics:\n  eval_loss: 0.4213\n  eval_runtime: 95.5946\n  eval_samples_per_second: 6.5900\n  eval_steps_per_second: 3.2950\n  epoch: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nğŸ§ª TESTING PREDICTIONS\n======================================================================\n\nTesting predictions:\n\nâœ“ Text: à¦›à¦¾à¦¤à§à¦°à¦°à¦¾ à¦¦à§‡à¦¶à§‡à¦° à¦­à¦¬à¦¿à¦·à§à¦¯à§...\n  Expected: Positive, Got: Positive\n\nâœ“ Text: à¦¸à¦°à¦•à¦¾à¦° à¦…à¦¤à§à¦¯à¦¾à¦šà¦¾à¦° à¦•à¦°à¦›à§‡...\n  Expected: Negative, Got: Negative\n\nâœ“ Text: à¦…à¦ªà¦°à¦¾à¦§à§‡à¦° à¦•à§‹à¦¨ à¦§à¦°à§à¦® à¦¨à§‡à¦‡...\n  Expected: Neutral, Got: Neutral\n\nâœ“ Text: à¦œà§à¦²à¦¾à¦‡ à¦¬à¦¿à¦ªà§à¦²à¦¬ à¦›à¦¿à¦² à¦à¦•à¦Ÿà¦¿ à¦à¦¤à¦¿à¦¹à¦¾à¦¸à¦¿à¦• à¦˜à¦Ÿà¦¨à¦¾...\n  Expected: Positive, Got: Positive\n\nâœ— Text: à¦¦à§à¦°à§à¦¨à§€à¦¤à¦¿ à¦¬à¦¨à§à¦§ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡...\n  Expected: Negative, Got: Positive\n\nAccuracy: 4/5 = 80.0%\n\n======================================================================\nğŸ‰ DONE!\n======================================================================\nğŸ’¾ Free disk space: 1422.00 GB\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"1422.0008583068848"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nGradio Interface for Finetuned Gemma-2B Bangla Sentiment Analysis\nSimple and beautiful UI for testing the model\n\"\"\"\n\nimport gradio as gr\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport time\n\n# ============================================================================\n# Load Model and Tokenizer\n# ============================================================================\nprint(\"ğŸ”„ Loading finetuned model...\")\n\nMODEL_PATH = \"./gemma2b_sentiment_final\"  # Change if your path is different\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    \n    # Setup padding\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    \n    # Load model\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_PATH,\n        torch_dtype=torch.bfloat16 if use_bf16 else torch.float32,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n    )\n    model.eval()\n    \n    print(f\"âœ… Model loaded successfully on {device}\")\n    print(f\"âœ… Using BF16: {use_bf16}\")\n\nexcept Exception as e:\n    print(f\"âŒ Error loading model: {e}\")\n    print(\"\\nğŸ’¡ Make sure the model path is correct and model is saved.\")\n    raise\n\n# ============================================================================\n# Prediction Function\n# ============================================================================\ndef predict_sentiment(text, show_confidence=False):\n    \"\"\"\n    Predict sentiment from Bangla text\n    \n    Args:\n        text: Input Bangla text\n        show_confidence: Whether to show confidence scores (experimental)\n    \n    Returns:\n        Sentiment label and optional confidence\n    \"\"\"\n    if not text or text.strip() == \"\":\n        return \"âš ï¸ à¦¦à¦¯à¦¼à¦¾ à¦•à¦°à§‡ à¦•à¦¿à¦›à§ à¦²à¦¿à¦–à§à¦¨ (Please enter some text)\", \"\"\n    \n    try:\n        # Format prompt\n        prompt = f\"Sentiment: {text.strip()}\\nLabel:\"\n        \n        # Tokenize\n        inputs = tokenizer(\n            prompt, \n            return_tensors=\"pt\", \n            truncation=True, \n            max_length=110\n        ).to(model.device)\n        \n        # Generate\n        start_time = time.time()\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0.01,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n                return_dict_in_generate=True,\n                output_scores=show_confidence,\n            )\n        \n        inference_time = time.time() - start_time\n        \n        # Decode\n        result = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n        label = result.split(\"Label:\")[-1].strip()\n        \n        # Clean up label\n        label = label.split()[0] if label else \"Unknown\"\n        \n        # Map to Bangla + English\n        label_map = {\n            \"Positive\": \"ğŸ˜Š Positive (à¦‡à¦¤à¦¿à¦¬à¦¾à¦šà¦•)\",\n            \"Negative\": \"ğŸ˜ Negative (à¦¨à§‡à¦¤à¦¿à¦¬à¦¾à¦šà¦•)\",\n            \"Neutral\": \"ğŸ˜ Neutral (à¦¨à¦¿à¦°à¦ªà§‡à¦•à§à¦·)\",\n        }\n        \n        display_label = label_map.get(label, f\"â“ {label}\")\n        \n        # Create info text\n        info = f\"â±ï¸ Inference time: {inference_time:.3f}s\"\n        \n        # Optional: Add confidence (experimental)\n        if show_confidence and hasattr(outputs, 'scores') and len(outputs.scores) > 0:\n            try:\n                # Get probabilities for first generated token\n                first_token_scores = outputs.scores[0][0]\n                probs = torch.nn.functional.softmax(first_token_scores, dim=-1)\n                confidence = probs.max().item() * 100\n                info += f\"\\nğŸ¯ Confidence: {confidence:.1f}%\"\n            except:\n                pass\n        \n        # Cleanup\n        del inputs, outputs\n        torch.cuda.empty_cache()\n        \n        return display_label, info\n        \n    except Exception as e:\n        return f\"âŒ Error: {str(e)}\", \"\"\n\n# ============================================================================\n# Example Texts\n# ============================================================================\nexamples = [\n    [\"à¦›à¦¾à¦¤à§à¦°à¦°à¦¾ à¦¦à§‡à¦¶à§‡à¦° à¦­à¦¬à¦¿à¦·à§à¦¯à§\"],\n    [\"à¦¸à¦°à¦•à¦¾à¦° à¦œà¦¨à¦—à¦£à§‡à¦° à¦‰à¦ªà¦° à¦…à¦¤à§à¦¯à¦¾à¦šà¦¾à¦° à¦•à¦°à¦›à§‡\"],\n    [\"à¦…à¦ªà¦°à¦¾à¦§à§‡à¦° à¦•à§‹à¦¨ à¦§à¦°à§à¦® à¦¨à§‡à¦‡\"],\n    [\"à¦œà§à¦²à¦¾à¦‡ à¦¬à¦¿à¦ªà§à¦²à¦¬ à¦›à¦¿à¦² à¦à¦•à¦Ÿà¦¿ à¦à¦¤à¦¿à¦¹à¦¾à¦¸à¦¿à¦• à¦˜à¦Ÿà¦¨à¦¾\"],\n    [\"à¦¦à§à¦°à§à¦¨à§€à¦¤à¦¿ à¦¬à¦¨à§à¦§ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡\"],\n    [\"à¦¶à¦¿à¦•à§à¦·à¦¾à¦°à§à¦¥à§€à¦°à¦¾ à¦†à¦¨à§à¦¦à§‹à¦²à¦¨à§‡ à¦…à¦‚à¦¶ à¦¨à¦¿à¦šà§à¦›à§‡\"],\n    [\"à¦†à¦®à¦°à¦¾ à¦¨à§à¦¯à¦¾à¦¯à¦¼à¦¬à¦¿à¦šà¦¾à¦° à¦šà¦¾à¦‡\"],\n    [\"à¦ªà§à¦²à¦¿à¦¶ à¦¨à¦¿à¦°à¦ªà¦°à¦¾à¦§ à¦®à¦¾à¦¨à§à¦·à¦•à§‡ à¦®à¦¾à¦°à¦›à§‡\"],\n    [\"à¦¦à§‡à¦¶à§‡ à¦¶à¦¾à¦¨à§à¦¤à¦¿ à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¿à¦¤ à¦¹à¦¯à¦¼à§‡à¦›à§‡\"],\n    [\"à¦à¦Ÿà¦¿ à¦à¦•à¦Ÿà¦¿ à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦˜à¦Ÿà¦¨à¦¾\"],\n]\n\n# ============================================================================\n# Gradio Interface\n# ============================================================================\ndef create_interface():\n    \"\"\"Create beautiful Gradio interface\"\"\"\n    \n    with gr.Blocks(\n        theme=gr.themes.Soft(),\n        title=\"Bangla Sentiment Analysis - July Revolution\"\n    ) as demo:\n        \n        # Header\n        gr.Markdown(\n            \"\"\"\n            # ğŸ‡§ğŸ‡© Bangla Sentiment Analysis\n            ### July Revolution Dataset - Finetuned Gemma-2B\n            \n            à¦à¦‡ à¦®à¦¡à§‡à¦²à¦Ÿà¦¿ à¦¬à¦¾à¦‚à¦²à¦¾ à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ à¦¥à§‡à¦•à§‡ Sentiment (à¦‡à¦¤à¦¿à¦¬à¦¾à¦šà¦•/à¦¨à§‡à¦¤à¦¿à¦¬à¦¾à¦šà¦•/à¦¨à¦¿à¦°à¦ªà§‡à¦•à§à¦·) à¦¶à¦¨à¦¾à¦•à§à¦¤ à¦•à¦°à§‡à¥¤  \n            This model detects sentiment (Positive/Negative/Neutral) from Bangla text.\n            \"\"\"\n        )\n        \n        with gr.Row():\n            with gr.Column(scale=2):\n                # Input\n                text_input = gr.Textbox(\n                    label=\"ğŸ“ à¦¬à¦¾à¦‚à¦²à¦¾ à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ à¦²à¦¿à¦–à§à¦¨ (Enter Bangla Text)\",\n                    placeholder=\"à¦à¦–à¦¾à¦¨à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦¬à¦¾à¦‚à¦²à¦¾ à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ à¦²à¦¿à¦–à§à¦¨...\",\n                    lines=4,\n                )\n                \n                # Options\n                with gr.Row():\n                    confidence_check = gr.Checkbox(\n                        label=\"Show confidence (experimental)\",\n                        value=False,\n                    )\n                \n                # Buttons\n                with gr.Row():\n                    submit_btn = gr.Button(\"ğŸ” Analyze Sentiment\", variant=\"primary\", size=\"lg\")\n                    clear_btn = gr.ClearButton([text_input], value=\"ğŸ—‘ï¸ Clear\")\n            \n            with gr.Column(scale=1):\n                # Output\n                sentiment_output = gr.Textbox(\n                    label=\"ğŸ“Š Sentiment Result\",\n                    interactive=False,\n                    lines=2,\n                )\n                \n                info_output = gr.Textbox(\n                    label=\"â„¹ï¸ Info\",\n                    interactive=False,\n                    lines=2,\n                )\n        \n        # Examples\n        gr.Markdown(\"### ğŸ“‹ Example Texts (à¦¨à¦®à§à¦¨à¦¾ à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ)\")\n        gr.Examples(\n            examples=examples,\n            inputs=text_input,\n            label=None,\n        )\n        \n        # Footer\n        gr.Markdown(\n            \"\"\"\n            ---\n            **Model:** Google Gemma-2B (Finetuned)  \n            **Dataset:** July Revolution Sentiment Analysis (4200 samples)  \n            **Labels:** Positive (à¦‡à¦¤à¦¿à¦¬à¦¾à¦šà¦•) | Negative (à¦¨à§‡à¦¤à¦¿à¦¬à¦¾à¦šà¦•) | Neutral (à¦¨à¦¿à¦°à¦ªà§‡à¦•à§à¦·)\n            \"\"\"\n        )\n        \n        # Event handlers\n        submit_btn.click(\n            fn=predict_sentiment,\n            inputs=[text_input, confidence_check],\n            outputs=[sentiment_output, info_output],\n        )\n        \n        text_input.submit(\n            fn=predict_sentiment,\n            inputs=[text_input, confidence_check],\n            outputs=[sentiment_output, info_output],\n        )\n    \n    return demo\n\n# ============================================================================\n# Launch\n# ============================================================================\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\"*70)\n    print(\"ğŸš€ Launching Gradio Interface...\")\n    print(\"=\"*70 + \"\\n\")\n    \n    demo = create_interface()\n    \n    # Launch with options\n    demo.launch(\n        share=True,  # Create public link\n        server_name=\"0.0.0.0\",  # Allow external access\n        server_port=7860,  # Default Gradio port\n        show_error=True,\n        # Uncomment below for authentication\n        # auth=(\"admin\", \"password123\"),\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T12:04:24.894721Z","iopub.execute_input":"2026-02-14T12:04:24.895037Z","iopub.status.idle":"2026-02-14T12:04:34.810835Z","shell.execute_reply.started":"2026-02-14T12:04:24.895008Z","shell.execute_reply":"2026-02-14T12:04:34.810211Z"}},"outputs":[{"name":"stdout","text":"ğŸ”„ Loading finetuned model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eda178906aa4426bf2007f7cd6131c8"}},"metadata":{}},{"name":"stdout","text":"âœ… Model loaded successfully on cuda\nâœ… Using BF16: True\n\n======================================================================\nğŸš€ Launching Gradio Interface...\n======================================================================\n\n* Running on local URL:  http://0.0.0.0:7860\n* Running on public URL: https://d48ddb14f0c4a387c6.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://d48ddb14f0c4a387c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":2}]}